"""Re-compute semantic entropy (RSE) for generated Reddit comments.

This script takes as input a `comments.json` produced by
`unstructured_tasks/inference/generate_comments_reddit.py`, recomputes the
per-article semantic entropy with a specified model (potentially different from
the one used originally), and writes the updated results to an output JSON file
with the same structure.
"""

from __future__ import annotations

import dataclasses
import json
import os
from typing import Any, Dict, List, Optional

import numpy as np
import tyro
import vllm
import dotenv

from unstructured_tasks.metrics import semantic_entropy


@dataclasses.dataclass
class Config:
    """Configuration for recomputing semantic entropy (RSE)."""

    # Path to an existing comments.json generated by generate_comments_reddit.py
    input_comments_path: str

    # Prompt for the entropy clustering model
    semantic_entropy_prompt_path: str = (
        "unstructured_tasks/metrics/semantic_entropy/prompts/semantic_clusters.txt"
    )

    # Model to use for (re-)computing semantic entropy
    model_semantic_entropy: str = "meta-llama/Llama-3.1-8B-Instruct"

    # Output file path (JSON). Will contain the same structure as the input
    # but with updated "entropies" and "mean_entropy" fields.
    output_path: str = "data/generated_responses/reddit_comments/rse/comments_rse.json"

    # VLLM options
    max_model_len: int = 4096
    gpu_memory_utilization: float = 0.8


def _load_json(path: str) -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def _save_json(path: str, data: Dict[str, Any]) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=4, ensure_ascii=False)


def _get_mode_key(data: Dict[str, Any]) -> str:
    """Return the top-level key that contains a dict with a "comments" field.

    Expected keys are typically "keywords" or "direct".
    """
    for key, value in data.items():
        if isinstance(value, dict) and "comments" in value:
            return key
    raise ValueError("Input JSON does not contain a valid comments structure.")


def _compute_entropies(
    comments_groups: List[List[str]],
    semantic_entropy_prompt: str,
    llm: Optional[vllm.LLM],
) -> List[float]:
    entropies: List[float] = []
    for group in comments_groups:
        entropy_value = semantic_entropy.interface.semantic_entropy_from_comments(
            group, semantic_entropy_prompt, llm
        )
        # Cast to float for JSON serializability and consistency
        entropies.append(float(entropy_value))
    return entropies


def main(cfg: Config) -> None:
    dotenv.load_dotenv()

    # Read input comments JSON (same structure as produced by generation script)
    data = _load_json(cfg.input_comments_path)

    # Load semantic entropy prompt
    with open(cfg.semantic_entropy_prompt_path, "r", encoding="utf-8") as f:
        semantic_entropy_prompt = f.read()

    # Prepare the model for entropy computation
    semantic_entropy_model = vllm.LLM(
        model=cfg.model_semantic_entropy,
        max_model_len=cfg.max_model_len,
        gpu_memory_utilization=cfg.gpu_memory_utilization,
    )

    # Identify the mode key (e.g., "keywords" or "direct") and the comments
    mode_key = _get_mode_key(data)
    comments_section: Dict[str, Any] = data[mode_key]

    comments_groups: List[List[str]] = comments_section["comments"]

    # Compute entropies per article/group
    entropies = _compute_entropies(
        comments_groups=comments_groups,
        semantic_entropy_prompt=semantic_entropy_prompt,
        llm=semantic_entropy_model,
    )

    comments_section["entropies"] = entropies
    comments_section["mean_entropy"] = float(np.mean(entropies)) if entropies else 0.0

    # Write updated results
    _save_json(cfg.output_path, data)


if __name__ == "__main__":
    import logging
#    logging.basicConfig(level=logging.DEBUG)
    cfg = tyro.cli(Config)
    main(cfg)

